
Transformation C: RandomHorizontalFlip, RandomRotation. Applies data augmentation
        transformations before basic transformations!!
    

Name = A series with double convolutional layers
Description = A CNN with 4 sets of consecutive convolutional layers where the first one doubles the channels and the second retains them.
            After each set, a dropout layer and a max pooling layer follow. In the end of the network there is a flatten layer with 8192 nodes.
            The last layers are linear droping the nodes, followed by dropout layers.
        
ConvolutionalNeuralNetwork(
  (_model): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Dropout2d(p=0.2, inplace=False)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU()
    (11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU()
    (14): Dropout2d(p=0.2, inplace=False)
    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (16): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): ReLU()
    (19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (21): ReLU()
    (22): Dropout2d(p=0.2, inplace=False)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU()
    (27): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU()
    (30): Dropout2d(p=0.2, inplace=False)
    (31): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (32): Flatten(start_dim=1, end_dim=-1)
    (33): Linear(in_features=8192, out_features=1024, bias=True)
    (34): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (35): Dropout(p=0.35, inplace=False)
    (36): Linear(in_features=1024, out_features=128, bias=True)
    (37): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Dropout(p=0.35, inplace=False)
    (39): Linear(in_features=128, out_features=1, bias=True)
  )
)
Epochs = 70
Learning rate = 0.001
Batch size = 128
Weight decay = 0.0001


Training samples: 22498
Validation samples: 1250
Early stopping: Enabled
Image size: 128x128

Transforms: StandardTransform
Transform: Compose(
               RandomHorizontalFlip(p=0.5)
               RandomRotation(degrees=[-30.0, 30.0], interpolation=InterpolationMode.NEAREST, expand=False, fill=0)
               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)
               ToTensor()
           )


Epoch [1/70]
Training Loss: 0.6557
Validation Loss: 0.5431
Training Accuracy: 0.6302
Validation Accuracy: 0.7312
Duration: 573.70 seconds


Epoch [2/70]
Training Loss: 0.5775
Validation Loss: 0.5073
Training Accuracy: 0.6994
Validation Accuracy: 0.7520
Duration: 394.76 seconds


Epoch [3/70]
Training Loss: 0.5258
Validation Loss: 0.4600
Training Accuracy: 0.7368
Validation Accuracy: 0.7736
Duration: 395.25 seconds


Epoch [4/70]
Training Loss: 0.4823
Validation Loss: 0.3911
Training Accuracy: 0.7693
Validation Accuracy: 0.8248
Duration: 396.05 seconds


Epoch [5/70]
Training Loss: 0.4314
Validation Loss: 0.3683
Training Accuracy: 0.7984
Validation Accuracy: 0.8352
Duration: 397.01 seconds


Epoch [6/70]
Training Loss: 0.3964
Validation Loss: 0.3106
Training Accuracy: 0.8231
Validation Accuracy: 0.8592
Duration: 396.67 seconds


Epoch [7/70]
Training Loss: 0.3681
Validation Loss: 0.2881
Training Accuracy: 0.8359
Validation Accuracy: 0.8712
Duration: 397.68 seconds


Epoch [8/70]
Training Loss: 0.3451
Validation Loss: 0.2586
Training Accuracy: 0.8438
Validation Accuracy: 0.8880
Duration: 372.78 seconds


Epoch [9/70]
Training Loss: 0.3215
Validation Loss: 0.2677
Training Accuracy: 0.8603
Validation Accuracy: 0.8824
Duration: 389.66 seconds


Epoch [10/70]
Training Loss: 0.3000
Validation Loss: 0.2616
Training Accuracy: 0.8706
Validation Accuracy: 0.8872
Duration: 390.86 seconds


Epoch [11/70]
Training Loss: 0.2905
Validation Loss: 0.2094
Training Accuracy: 0.8733
Validation Accuracy: 0.9128
Duration: 375.74 seconds


Epoch [12/70]
Training Loss: 0.2756
Validation Loss: 0.2442
Training Accuracy: 0.8834
Validation Accuracy: 0.8928
Duration: 385.85 seconds


Epoch [13/70]
Training Loss: 0.2590
Validation Loss: 0.1936
Training Accuracy: 0.8907
Validation Accuracy: 0.9240
Duration: 388.96 seconds


Epoch [14/70]
Training Loss: 0.2477
Validation Loss: 0.1921
Training Accuracy: 0.8949
Validation Accuracy: 0.9168
Duration: 389.09 seconds


Epoch [15/70]
Training Loss: 0.2404
Validation Loss: 0.1659
Training Accuracy: 0.8980
Validation Accuracy: 0.9424
Duration: 388.85 seconds


Epoch [16/70]
Training Loss: 0.2273
Validation Loss: 0.1731
Training Accuracy: 0.9050
Validation Accuracy: 0.9384
Duration: 388.35 seconds


Epoch [17/70]
Training Loss: 0.2249
Validation Loss: 0.1723
Training Accuracy: 0.9085
Validation Accuracy: 0.9320
Duration: 388.62 seconds


Epoch [18/70]
Training Loss: 0.2212
Validation Loss: 0.1656
Training Accuracy: 0.9082
Validation Accuracy: 0.9352
Duration: 387.55 seconds


Epoch [19/70]
Training Loss: 0.2148
Validation Loss: 0.1557
Training Accuracy: 0.9105
Validation Accuracy: 0.9408
Duration: 388.18 seconds


Epoch [20/70]
Training Loss: 0.2046
Validation Loss: 0.1846
Training Accuracy: 0.9139
Validation Accuracy: 0.9208
Duration: 387.65 seconds


Epoch [21/70]
Training Loss: 0.2034
Validation Loss: 0.1427
Training Accuracy: 0.9156
Validation Accuracy: 0.9448
Duration: 388.14 seconds


Epoch [22/70]
Training Loss: 0.2016
Validation Loss: 0.1525
Training Accuracy: 0.9167
Validation Accuracy: 0.9376
Duration: 387.28 seconds


Epoch [23/70]
Training Loss: 0.1918
Validation Loss: 0.1502
Training Accuracy: 0.9211
Validation Accuracy: 0.9408
Duration: 387.05 seconds


Epoch [24/70]
Training Loss: 0.1924
Validation Loss: 0.1580
Training Accuracy: 0.9204
Validation Accuracy: 0.9376
Duration: 386.44 seconds


Epoch [25/70]
Training Loss: 0.1865
Validation Loss: 0.1300
Training Accuracy: 0.9207
Validation Accuracy: 0.9448
Duration: 386.50 seconds


Epoch [26/70]
Training Loss: 0.1835
Validation Loss: 0.1368
Training Accuracy: 0.9245
Validation Accuracy: 0.9448
Duration: 386.48 seconds


Epoch [27/70]
Training Loss: 0.1825
Validation Loss: 0.1587
Training Accuracy: 0.9229
Validation Accuracy: 0.9328
Duration: 386.65 seconds


Epoch [28/70]
Training Loss: 0.1754
Validation Loss: 0.1361
Training Accuracy: 0.9272
Validation Accuracy: 0.9384
Duration: 387.13 seconds


Epoch [29/70]
Training Loss: 0.1761
Validation Loss: 0.1936
Training Accuracy: 0.9270
Validation Accuracy: 0.9200
Duration: 541.79 seconds


Epoch [30/70]
Training Loss: 0.1760
Validation Loss: 0.1648
Training Accuracy: 0.9270
Validation Accuracy: 0.9296
Duration: 402.40 seconds


Epoch [31/70]
Training Loss: 0.1670
Validation Loss: 0.1422
Training Accuracy: 0.9315
Validation Accuracy: 0.9408
Duration: 386.38 seconds


Early stopping triggered after epoch 31.



Training duration: 12399.55 seconds
