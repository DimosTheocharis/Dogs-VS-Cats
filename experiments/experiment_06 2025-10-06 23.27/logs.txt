
Let's try data augmentation, model 1. This is the first model that uses: RandomHorizontalFlip, RandomAffine(zoom in/out). But with weaker transformations

Name = A series with double convolutional layers
Description = A CNN with 4 sets of consecutive convolutional layers where the first one doubles the channels and the second retains them.
            After each set, a dropout layer and a max pooling layer follow. In the end of the network there is a flatten layer with 8192 nodes.
            The last layers are linear droping the nodes, followed by dropout layers.
        
ConvolutionalNeuralNetwork(
  (_model): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Dropout2d(p=0.2, inplace=False)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU()
    (11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU()
    (14): Dropout2d(p=0.2, inplace=False)
    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (16): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): ReLU()
    (19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (21): ReLU()
    (22): Dropout2d(p=0.2, inplace=False)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU()
    (27): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU()
    (30): Dropout2d(p=0.2, inplace=False)
    (31): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (32): Flatten(start_dim=1, end_dim=-1)
    (33): Linear(in_features=8192, out_features=1024, bias=True)
    (34): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (35): Dropout(p=0.35, inplace=False)
    (36): Linear(in_features=1024, out_features=128, bias=True)
    (37): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Dropout(p=0.35, inplace=False)
    (39): Linear(in_features=128, out_features=1, bias=True)
  )
)
Epochs = 70
Learning rate = 0.001
Batch size = 128
Weight decay = 0.0001


Training samples: 22498
Validation samples: 1250
Early stopping: Enabled
Image size: 128x128

Transforms: StandardTransform
Transform: Compose(
               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)
               ToTensor()
               RandomHorizontalFlip(p=0.3)
               RandomAffine(degrees=[0.0, 0.0], scale=(0.1, 0.1), interpolation=InterpolationMode.NEAREST, fill=0)
           )


Epoch [1/70]
Training Loss: 0.7027
Validation Loss: 0.7641
Training Accuracy: 0.5442
Validation Accuracy: 0.5416
Duration: 606.10 seconds


Epoch [2/70]
Training Loss: 0.6631
Validation Loss: 0.7057
Training Accuracy: 0.6067
Validation Accuracy: 0.5400
Duration: 516.01 seconds


Epoch [3/70]
Training Loss: 0.6330
Validation Loss: 0.6905
Training Accuracy: 0.6466
Validation Accuracy: 0.5432
Duration: 359.75 seconds


Epoch [4/70]
Training Loss: 0.6098
Validation Loss: 0.7072
Training Accuracy: 0.6731
Validation Accuracy: 0.5016
Duration: 351.68 seconds


Epoch [5/70]
Training Loss: 0.5914
Validation Loss: 0.7073
Training Accuracy: 0.6902
Validation Accuracy: 0.4880
Duration: 355.07 seconds


Epoch [6/70]
Training Loss: 0.5744
Validation Loss: 0.7325
Training Accuracy: 0.7046
Validation Accuracy: 0.4864
Duration: 353.04 seconds


Epoch [7/70]
Training Loss: 0.5667
Validation Loss: 0.7604
Training Accuracy: 0.7087
Validation Accuracy: 0.4720
Duration: 352.70 seconds


Epoch [8/70]
Training Loss: 0.5571
Validation Loss: 0.7572
Training Accuracy: 0.7175
Validation Accuracy: 0.4824
Duration: 353.71 seconds


Epoch [9/70]
Training Loss: 0.5533
Validation Loss: 0.7117
Training Accuracy: 0.7206
Validation Accuracy: 0.4736
Duration: 385.31 seconds


Early stopping triggered after epoch 9.



Training duration: 3633.39 seconds
